Our paper comprises three experiments: a motivating experiment, an evaluation of our algorithm through simulation, and an evaluation via a cloud experiment. In this document, we provide a detailed description of the setup and expected results for each experiment. We provide the code in XXXXXXXX.

### Motivating Experiment

The content of this section corresponds to Sec. 3.2 of the paper. To validate our assumption of gradient coding (GC) that while GC can save delays caused by stragglers, it may increase the overall training time since GC requires each worker to evaluate the gradients on multiple dataset partitions, we conduct this motivating experiment on the Google Cloud Platform (GCP) using $12$ worker nodes and $1$ master node. We train the ResNet-18 model for one epoch on the CIFAR-10 dataset using standard distributed gradient descent (DGD) and GC. Specifically, we implement GC on two dataset partition placement schemes: fraction repetition (FR) and cyclic repetition (CR). Furthermore, since the number of dataset partitions on each worker, $\textit{i.e.}$, $c$, directly affects the computation time of each worker, we implement three different parameters, $\textit{i.e.}$, $c=2,3,4$, for each GC-FR and GC-CR. In DGD, each worker only evaluates the gradients on one single dataset partition, so $c=1$ in DGD. We use the learning rate $0.01$ and batch size $64$. Since CIFAR-10 has $50000$ training images, there are 782 steps in one epoch training.

To ensure fair comparisons among different training schemes, we carefully control all random operations, and the models are initialized with the same parameters at the beginning. We have also attached an initialized model, and it is necessary to place this model in the same directory. If not, the program would report an error saying there is no initialized model. In this case, we can generate a new initialized model by hiding the `load()` in the code and running the training for $1$ step only. Remember to use `save()` to save the initialized model. We used the same methods in subsequent evaluations to ensure a fair comparison.

Based on the same random seed and initialized model, it is expected that all schemes achieve the same loss after $1$ epoch training since both DGD and GC use the full gradients to update the model in each step. We categorize stragglers into two types: ordinary stragglers and extreme stragglers. Ordinary stragglers cause prolonged latency but eventually upload their results to the master, whereas extreme stragglers become completely unavailable, thus blocking the entire training process. Ordinary stragglers are more common than extreme stragglers. We believe that although ordinary stragglers may delay training in some steps, the delay is averaged out over a large number of steps. Therefore, since $c=1$ in DGD, DGD should be the fastest in training, provided that no extreme stragglers occur during the process. With a larger value of $c$ in GC, the training time should also increase. Specifically, GC-FR should outperform GC-CR due to the different coding complexity. Figure 3 in the paper conforms to all our expectations above. 

### Simulation

The content of this section corresponds to Sec. 6.2 of the paper. To evaluate the proposed pipelined gradient training (PGC), we run a simulation in our local high-performance computer (HPC) with $n=12$ workers. In this simulation, we train the ResNet-18 model for one epoch on the ImageNet dataset using DGD, GC, PGC, ignore-straggler stochastic gradient descent (IS-SGD), and ignore-straggler gradient coding (IS-GC). Since the original ImageNet is too large, we use the downsampled version with $32\times 32$ pixels for each image. To observe the change in the performance with an increasing number of stragglers, $\textit{i.e.}$, $s$, we simulate three straggler schemes by setting $s=1, 2$, and $3$. The batch size and learning rate are 256 and 0.01, respectively. 

We first compare the time consumption per step. The result is presented in Figure 7 in the paper. When simulating the distributed training locally, there are no stragglers. Therefore, to simulate stragglers, we introduce additional random delays following exponential distributions with the means equalling 1 and 2 seconds for each worker, respectively. The result of DGD is the baseline in our comparison. For each $s$, since GC requires evaluating the gradient on multiple dataset partitions, GC should take the longest time per step. With an increasing value of $s$, GC requires evaluating more dataset partitions for each worker, so the time per step will also increase. As for IS-GC, we fix $c=2$ which is the lowest possible value of $c$ in IS-GC for all $s$. Therefore, the computation time for each worker in IS-GC should be similar under different $s$. However, with the increasing value of $s$, the master receives fewer workers for decoding, so the time per step of IS-GC will decrease. Similarly, IS-SGD shares the same trend as IS-GC, but it costs less time per step due to $c=1$ only. The performance of PGC is expected to be slightly slower than IS-SGD because although PGC computes only one dataset partition at a time, IS-SGD does not involve additional coding time. PGC should be faster than IS-GC because although both require the master to accept the same number of workers, the number of dataset partitions each worker processes is greater with PGC.

We then demonstrate the loss reduction over the training steps in Figure 8 in the paper. We train each scheme for 25 epochs and compare the loss reduction. Since we theoretically show that PGC does not hurt convergence, the model using PGC should be converged like other schemes. Moreover, PGC leverages some stale gradients in each step so that it may converge faster than other schemes, according to the observation in asynchronous learning.

### Experiments in the Cloud

The content of this section corresponds to Sec. 6.3 of the paper. We also evaluate the proposed PGC on GCP. Since GCP is a real cloud environment, we no longer add additional delays to simulate stragglers. We use the same distributed computing cluster as our motivating experiments. We still train the Resnet18 model on the CIFAR-10 dataset until the training loss reaches a threshold of 0.5 using DGD, GC, IS-GC, IS-SGD, and PGC. We set the batch size as 256 and the learning rate as 0.01.

Similarly, we first demonstrate the time consumption. The expected result should be the same as the simulation. We also record the number of steps to reach the loss threshold. Since both DGD and GC use the full gradients to update the model in each step, they share the same number of steps to reach the loss threshold. As we discussed in the simulation, IS-SGD and PGC leverage partial stale gradients to update the model so they should use fewer steps to reach the same loss threshold.  As for the loss reduction over time, since PGC has the shortest time per step and requires the fewest number of steps, it should perform significantly better than other schemes. The results are presented in Figures 9 and 10, respectively, in the paper.

